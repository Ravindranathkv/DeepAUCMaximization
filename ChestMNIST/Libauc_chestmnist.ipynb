{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LibAUC Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libauc.models import resnet18\n",
    "from libauc.losses import AUCMLoss\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as torch_data\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import medmnist\n",
    "from medmnist import INFO, Evaluator\n",
    "\n",
    "\n",
    "from libauc.losses import AUCMLoss, CrossEntropyLoss\n",
    "from libauc.optimizers import PESG, Adam\n",
    "from libauc.models import densenet121 as DenseNet121\n",
    "from libauc.datasets import CheXpert\n",
    "\n",
    "import torch \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_flag = 'pathmnist'\n",
    "data_flag = 'chestmnist'\n",
    "download = True\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 8\n",
    "lr = 0.001\n",
    "\n",
    "info = INFO[data_flag]\n",
    "task = info['task']\n",
    "n_channels = info['n_channels']\n",
    "n_classes = len(info['label'])\n",
    "\n",
    "DataClass = getattr(medmnist, info['python_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'multi-label, binary-class'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using downloaded and verified file: C:\\Users\\Hari\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Hari\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Hari\\.medmnist\\chestmnist.npz\n",
      "Using downloaded and verified file: C:\\Users\\Hari\\.medmnist\\chestmnist.npz\n"
     ]
    }
   ],
   "source": [
    "data_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Grayscale(3),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(0.5, 0.5),\n",
    "        ])\n",
    "\n",
    "# load the data\n",
    "train_dataset = DataClass(split='train', transform=data_transform, download=download)\n",
    "val_dataset = DataClass(split='val', transform=data_transform, download=download)\n",
    "test_dataset = DataClass(split='test', transform=data_transform, download=download)\n",
    "\n",
    "pil_dataset = DataClass(split='train', download=download)\n",
    "\n",
    "# encapsulate data into dataloader form\n",
    "train_loader = torch_data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader_at_eval = torch_data.DataLoader(dataset=train_dataset, batch_size=2*BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch_data.DataLoader(dataset=test_dataset, batch_size=2*BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01 # using smaller learning rate is better\n",
    "epoch_decay = 2e-5\n",
    "weight_decay = 1e-7\n",
    "margin = 1.0\n",
    "\n",
    "model = resnet18(num_classes=14)\n",
    "model = model.cuda()\n",
    "# criterion = nn.CrossEntropyLoss()  \n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = AUCMLoss()\n",
    "optimizer = PESG(model, \n",
    "                 loss_fn=criterion, \n",
    "                 lr=lr, \n",
    "                 margin=margin, \n",
    "                 epoch_decay=epoch_decay, \n",
    "                 weight_decay=weight_decay)\n",
    "CE_loss = nn.CrossEntropyLoss()\n",
    "# training\n",
    "best_val_auc = 0 \n",
    "for epoch in range(1001):\n",
    "    train_losses = []\n",
    "    for idx, data in enumerate(train_loader):\n",
    "      print(\"Iteration started\")\n",
    "      train_data, train_labels = data\n",
    "      train_data, train_labels  = train_data.cuda(), train_labels.cuda()\n",
    "      y_pred = model(train_data)\n",
    "      y_pred = torch.sigmoid(y_pred)\n",
    "      loss = criterion(y_pred, train_labels.type(torch.LongTensor).cuda())\n",
    "      train_losses.append(loss.item()/len(data))\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      break\n",
    "\n",
    "    print(\"Epoch : {:03d}  Train Loss : {:.5f} \".format(epoch, np.mean(train_losses)), end='')\n",
    "    model.eval()\n",
    "    with torch.no_grad():    \n",
    "        test_pred = []\n",
    "        test_true = [] \n",
    "        test_losses = []\n",
    "        test_CE_losses = []\n",
    "        for jdx, data in enumerate(test_loader):\n",
    "            test_data, test_labels = data\n",
    "            test_data = test_data.cuda()\n",
    "            y_pred = model(test_data)\n",
    "            test_pred.append(y_pred.cpu().detach().numpy())\n",
    "            test_true.append(test_labels.numpy())\n",
    "            test_losses.append(criterion(y_pred, test_labels.squeeze().type(torch.LongTensor).cuda()).item() / len(data))\n",
    "            # test_CE_losses.append(CE_loss(y_pred, test_labels.squeeze().float().cuda()).cpu())\n",
    "\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        val_auc_mean = 0\n",
    "        for i in range(14):\n",
    "            val_auc_mean +=  roc_auc_score(test_true[:,i], test_pred[:,i]) \n",
    "        val_auc_mean/=14\n",
    "        print(\"Val Loss : {:.5f}   \".format(np.mean(test_losses)), end = '')\n",
    "        model.train()\n",
    "\n",
    "        if best_val_auc < val_auc_mean:\n",
    "            best_val_auc = val_auc_mean\n",
    "            torch.save(model.state_dict(), 'pretrained_model.pth')\n",
    "\n",
    "        print ('BatchID= {}   Val_AUC={:.4f}   Best_Val_AUC={:.4f}'.format(\n",
    "            idx, val_auc_mean, best_val_auc ))\n",
    "          \n",
    "    print(\"Epoch : {}\".format(epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22433, 14)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isr_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
